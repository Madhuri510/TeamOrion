{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83fb579e",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This notebook demonstrates the process of extracting abstracts from the PubMed website and performing the necessary preprocessing on the extracted data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483714aa",
   "metadata": {},
   "source": [
    "# Imports\n",
    "- **requests**: To make HTTP requests to the PubMed API.\n",
    "- **pandas**: Manages large datasets in tabular format.\n",
    "- **csv**: To read/write data to CSV files.\n",
    "- **os**: To interact with the file system.\n",
    "- **xml.etree.ElementTree**: To parse and process XML responses.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62db88a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import xml.etree.ElementTree as ET\n",
    "import csv\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda15b6",
   "metadata": {},
   "source": [
    "# Defining a function to search PubMed using its API for article IDs based on a list of keywords. \n",
    "\n",
    "1. Constructs a search query from keywords using AND logic.\n",
    "2. Sends an HTTP request to PubMed's ESearch API.\n",
    "3. Parses the JSON response to extract a list of PubMed IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a49cf8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched PubMed IDs: ['39410854', '39403930', '39384801', '39384243', '39382658', '39377592', '39377544', '39375818', '39375657', '39368044']\n",
      "9999\n"
     ]
    }
   ],
   "source": [
    "def search_pubmed_for_ids(keywords):\n",
    "    # Construct the query using the keywords\n",
    "    query = ' AND '.join(keywords)\n",
    "    search_url = f\"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pubmed&term={query}&retmode=json&retmax=10000\"\n",
    "    \n",
    "    # Perform the request\n",
    "    response = requests.get(search_url)\n",
    "    \n",
    "    # Check for successful response\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        return data['esearchresult']['idlist']  # Return list of PubMed IDs\n",
    "    else:\n",
    "        print(f\"Failed to retrieve PubMed IDs. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "keywords = [\"Neoplasms\", \"Antineoplastic Agents\", \"Adverse Effects\", \"Toxicity\"]\n",
    "\n",
    "# Fetch PubMed IDs based on the keywords\n",
    "abstract_ids = search_pubmed_for_ids(keywords)\n",
    "\n",
    "# Display a few PubMed IDs to confirm\n",
    "print(\"Fetched PubMed IDs:\", abstract_ids[:10])  # Display first 10 PubMed IDs\n",
    "print(len(abstract_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706e983d",
   "metadata": {},
   "source": [
    "# Define the base URL and parameters for the EFetch API, which retrieves full article abstracts by PubMed IDs.\n",
    "\n",
    "1. base_url: The API endpoint for fetching articles.\n",
    "2. params: Default parameters for the API request (database, return type, API key)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc3c77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi\"\n",
    "\n",
    "# Parameters for the API request\n",
    "params = {\n",
    "    'db': 'pubmed',\n",
    "    'rettype': 'abstract',\n",
    "    'api_key': 'd4a0e5f85881f5f38b9c0e9a84ac5338e408'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942732cd",
   "metadata": {},
   "source": [
    "# Extracting the abstracts from PubMed using their IDs\n",
    "\n",
    "Set up a CSV file for saving PubMed abstracts and associated MeSH headings.\n",
    "\n",
    "1. Initialize counters for tracking total, null, and valid abstracts processed.\n",
    "2. Check if the CSV file already exists to avoid overwriting.\n",
    "3. Open the file in append or write mode and add a header if it's a new file.\n",
    "\n",
    "\n",
    "Define a function to fetch the abstract and MeSH headings for a given PubMed ID.\n",
    "\n",
    "1. Sends a request to the EFetch API using the PubMed ID.\n",
    "2. Parses the XML response to extract:\n",
    "   -  **AbstractText**: The article's abstract.\n",
    "   - **MeshHeading/DescriptorName**: Associated MeSH headings.\n",
    "3. Handles XML parsing errors and API response issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1659c72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Accessed Abstracts: 9999\n",
      "Valid abstracts: 9648\n",
      "Null abstracts: 351\n"
     ]
    }
   ],
   "source": [
    "total_abstract_count = 0\n",
    "null_abstract_count = 0\n",
    "valid_abstract_count = 0\n",
    "\n",
    "# Check if the CSV file already exists\n",
    "file_exists = os.path.isfile('raw_abstracts_data.csv')\n",
    "\n",
    "# Open the CSV file once to write all abstracts\n",
    "with open('raw_abstracts_data.csv', mode='a' if file_exists else 'w', newline='', encoding='utf-8') as csv_file:\n",
    "    writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row only if the file is being created (i.e., it does not already exist)\n",
    "    if not file_exists:\n",
    "        writer.writerow(['PubMedID', 'Abstract', 'Mesh Headings'])\n",
    "        \n",
    "    # Iterate through each PubMed ID and fetch the abstract\n",
    "    for pubmed_id in abstract_ids:\n",
    "        params['id'] = pubmed_id\n",
    "        response = requests.get(base_url, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            total_abstract_count += 1\n",
    "\n",
    "            # Parse the XML content and handle any parsing errors\n",
    "            try:\n",
    "                root = ET.fromstring(response.text)\n",
    "\n",
    "                # Find all elements with the tag 'AbstractText'\n",
    "                abstract_text_elements = root.findall('.//AbstractText')\n",
    "\n",
    "                # Concatenate all the text from <AbstractText> tags into one string\n",
    "                abstract_texts = ' '.join(''.join(element.itertext()).strip() for element in abstract_text_elements if element.text)\n",
    "                \n",
    "                # Find all the MeshHeadings\n",
    "                mesh_heading_elements = root.findall('.//MeshHeading/DescriptorName')\n",
    "                mesh_headings = ', '.join([element.text for element in mesh_heading_elements if element.text])\n",
    "\n",
    "                # Write the PubMed ID and combined abstract into the CSV\n",
    "                if abstract_texts:  # Only write if abstract is not null\n",
    "                    writer.writerow([pubmed_id, abstract_texts, mesh_headings])\n",
    "                    valid_abstract_count += 1\n",
    "                else:\n",
    "                    null_abstract_count += 1\n",
    "                \n",
    "            except ET.ParseError as e:\n",
    "                print(f\"XML parsing error for PubMed ID {pubmed_id}: {e}\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Failed to fetch data for PubMed ID {pubmed_id}. Status code: {response.status_code}\")\n",
    "\n",
    "print('Total Accessed Abstracts:', total_abstract_count)\n",
    "print('Valid abstracts:', valid_abstract_count)\n",
    "print('Null abstracts:', null_abstract_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f840d7cd",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Necessary Libraries and data.\n",
    "- **re**: For regular expressions to clean text (e.g., removing URLs and special characters).\n",
    "- **nltk**: Provides natural language processing tools.\n",
    "- **WordNetLemmatizer**: For reducing words to their base form.\n",
    "- **stopwords and word_tokenize**: To remove common words and tokenize text into words.\n",
    "- **punkt(NLTK)**: Used for word tokenization.\n",
    "- **stopwords(NLTK)**: Provides a list of common words to filter out.\n",
    "- **wordnet(NLTK)**: A lexical database for lemmatization.\n",
    "\n",
    "Preprocessing steps:\n",
    "1. **Convert to lowercase**: Standardizes text for consistent processing.\n",
    "2. **Remove URLs**: Cleans out links from the text.\n",
    "3. **Remove mathematical formulas**: Filters out numeric and symbolic formulas.\n",
    "4. **Remove special characters**: Keeps only letters, numbers, and spaces.\n",
    "5. **Tokenize**: Splits text into individual words.\n",
    "6. **Lemmatize**: Reduces words to their base or dictionary form (e.g., \"running\" â†’ \"run\").\n",
    "7. **Rejoin Tokens**: Combines the cleaned words back into a single string.\n",
    "\n",
    "Save the cleaned dataset to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5ec160",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "\n",
    "# Ensure you have the necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "df = pd.read_csv('raw_abstracts_data.csv')\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Load stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Define a preprocessing function\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "    # Remove mathematical formulas (assuming they include digits, symbols like +, -, /, *, etc.)\n",
    "    text = re.sub(r'[0-9]+[+-/*^=<>]+[0-9]*', '', text)\n",
    "\n",
    "    # Remove special characters (keep only alphanumeric and space)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "\n",
    "    # Tokenization (split into words)\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Lemmatize the tokens (convert to base forms)\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join the cleaned tokens back into a string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Apply preprocessing to the 'Abstract' column\n",
    "df['Abstract'] = df['Abstract'].apply(preprocess_text)\n",
    "\n",
    "# Save the cleaned data to a new CSV file\n",
    "df.to_csv('preprocessed_abstracts_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
